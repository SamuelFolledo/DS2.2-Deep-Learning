{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 2.2 Deep Learning: \n",
    "- **Make School Courses repo**: click [here](https://github.com/Make-School-Courses/DS-2.2-Deep-Learning)\n",
    "- **My GitHub Repo**: click [here](https://github.com/SamuelFolledo/DS2.2-Deep-Learning)\n",
    "\n",
    "\n",
    "**Disclaimer:** Table of Contents only works on Jupyter Notebook\n",
    "\n",
    "## Table of Contents\n",
    "1. **[Introduction to Deep Learning](#day1)**\n",
    "2. **[What is Neural Network](#day2)**\n",
    "3. **[Neural Network from Scratch](#day3)**\n",
    "4. **[Intro To Keras](#day4)**\n",
    "5. **[Backward Propagation](#day5)** - 11/9/2020\n",
    "6. **[](#day6)**\n",
    "7. **[](#day7)**\n",
    "8. **[Convolutional Neural Network](#day8)** - 11/18/2020\n",
    "    - [Lab 1](#lab1)\n",
    "9. **[Quiz and Lab day](#day9)**\n",
    "10. **[Recurrent Neural Network](#day10)** 11/30/2020\n",
    "11. **[](#day11)**\n",
    "12. **[](#day12)**\n",
    "13. **[](#day13)**\n",
    "14. **[](#day14)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day1\"></a> \n",
    "# Day 1: Introduction to Deep Learning\n",
    "https://docs.google.com/presentation/d/1vxQ_R8-gEtR896EoC1_ObSKoWJZje40H8wcXzdltjpo/edit#slide=id.g9e8c78a5a9_0_18\n",
    "\n",
    "### What We’re Going to Learn\n",
    "- High Level Overview of Deep Learning\n",
    "- Applications of Deep Learning \n",
    "- What is a Neural Network\n",
    "\n",
    "\n",
    "## Deep Learning\n",
    "- Deep learning is a **subfield of machine learning**\n",
    "- Deep learning is **all about neural networks**\n",
    "- Deep learning **refers to large neural networks**\n",
    "- Neural networks have been around for a while but is only relatively recently that we have the computing power to train and run the these larger more complex neural networks\n",
    "- The deep part comes from **having many layers in the networks**\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-1.png?raw=true\" width=\"700\">\n",
    "\n",
    "### Why Deep Learning?\n",
    "*“The analogy to deep learning is that the rocket engine is the deep learning models and the fuel is the huge amounts of data we can feed to these algorithms.”* - Andrew Ng\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-2.png?raw=true\" width=\"500\">\n",
    "\n",
    "### Drawbacks of Deep Learning\n",
    "- Interpretability is harder, very black box \n",
    "- Doesn’t work well with small datasets\n",
    "    - the less data you have, the worse it's going to perform\n",
    "    - more instances/samples (~10,000) not how many features\n",
    "- Needs lots of computing power\n",
    "\n",
    "## Neural Networks\n",
    "- Modeled loosely on the human brain\n",
    "- Composed of many simple processing nodes or “neurons”\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-3.png?raw=true\" width=\"700\">\n",
    "\n",
    "## A Neuron\n",
    "- For most modern deep learning algorithms we use what is called a “sigmoid neuron”\n",
    "- In order to understand these better let’s take a look at the idea they were based on, a perceptron, one of the first types of artificial neurons\n",
    "\n",
    "## Perceptrons\n",
    "- A perceptron takes in multiple inputs and produces a single binary output\n",
    "- Each of these inputs x have a weight w, what would weights do?\n",
    "\t◦ Weights are used to add importance of a feature, giving it more influence on the output\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-4.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "## Deep Neural Networks\n",
    "We add all those x*w’s together to get a sum\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-5.png?raw=true\" width=\"500\">\n",
    "\n",
    "\n",
    "## Activation Function //58m\n",
    "How do we know whether to give a 0 or 1, that’s the activation functions job! One example is the Unit Step Activation Function\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-6.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "### What are some drawbacks of this decision making model? //1h1m\n",
    "- Setting an absolute threshold is very not a good decision￼\n",
    "\n",
    "\n",
    "## Sigmoid Neuron: Much Less Harsh //1h4m\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-7.png?raw=true\" width=\"700\">\n",
    "\n",
    "## Sigmoid Neuron Output //1h5\n",
    "\n",
    "- Our output is no longer 0 or 1\n",
    "- It is now a value from 0 - 1 and can be interpreted like a probability\n",
    "- We now have more control over how that output is interpreted\n",
    "\n",
    "## Putting it all together\n",
    "- A neural network is really just a bunch of interconnected neurons, the output from one layer or neurons can then be used as one the inputs for the next layer\n",
    "- Through a series of layers different patterns of neurons are “activated” producing different end results for different inputs\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-8.png?raw=true\" width=\"700\">\n",
    "\n",
    "## Next Steps\n",
    "- Math happening behind the scenes?\n",
    "- What about those algorithms?\n",
    "- How does it actually get the inputs (features)?\n",
    "- What sorts of parameters and choices can we mess with, how do we optimize?\n",
    "- How do we choose how many layers and neurons?\n",
    "- What tools are available to us?\n",
    "- How can we apply this to a real world problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day2\"></a> \n",
    "# Day 2 What is Neural Network?\n",
    "https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Lessons/WhatisNeuralNetwork.md\n",
    "\n",
    "### Learning Objectives\n",
    "\t•\tWhat is Neural network\n",
    "\t•\tWe learn Forward and backward propagation in NN\n",
    "\t•\tWe implement a NN entirely in numpy\n",
    "\n",
    "## What is Neural Network?\n",
    "It is a **computational system inspired by the Structure, Processing Method and Learning Ability similar to our biological brain**\n",
    "\n",
    "### Characteristics of Artificial Neural Networks\n",
    "\t•\tA large number of very simple processing neuron-like processing elements\n",
    "\t•\tA large number of weighted connections between the elements\n",
    "\t•\tDistributed representation of knowledge over the connections\n",
    "\t•\tKnowledge is acquired by network through a learning process\n",
    "\n",
    "## What is perceptron?\n",
    "\t•\tA perceptron can be understood as anything that **takes multiple inputs and produces one output**\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day2-1.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "## Multi-layer perceptron (MLP)\n",
    "\t•\tMLP is the stack of perceptrons\n",
    "    \n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day2-2.png?raw=true\" width=\"700\">\n",
    "\n",
    "\t•\tIn this image, the yellow nodes are inputs, the blue nodes (at each vertical) are hidden layers and the orange ones are output of the MLP\n",
    "\n",
    "## Forward and backward propagation\n",
    "NN **takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer.** This result estimation process is technically known as **“Forward Propagation“**\n",
    "\n",
    "Next, we **compare the result with actual output**. The task is to make the output to neural network as close to actual (desired) output. This defines our cost function.\n",
    "We try to **obtain the weight of neurons such that the NN total error (our cost function) being minimized**. This process is known as **“Backward Propagation“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#creating a vector\n",
    "vector = np.array([5,2,1])\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98328823]\n",
      " [0.97397276]\n",
      " [0.0367804 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/\n",
    "# Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "\n",
    "#Sigmoid Function //gives us a value between 0 and 1 or Probability\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "\n",
    "#Variable initialization\n",
    "epoch=5000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "    D = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = D * slope_output_layer\n",
    "    Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *lr\n",
    "    bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "    wh += X.T.dot(d_hiddenlayer) *lr\n",
    "    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we update the weights to minimize the error?\n",
    "1. First we should define the cost function. for our example here the MSE is our cost function:\n",
    "\n",
    "$E= \\frac{1}{2} ({\\bf y}_t - {\\bf y}_p)^T ({\\bf y}_t - {\\bf y}_p)$\n",
    "\n",
    "2. We update the weight (${\\bf W}_i$ and ${\\bf W}_h$) such that the error, $E$, being minimized. The most popular algorithm is Gradient Descent:\n",
    "\n",
    "${\\bf W}_h = {\\bf W}_h + \\eta {\\partial E}/{\\partial {\\bf W}_h} $\n",
    "\n",
    "For our above example we can show that:\n",
    "\n",
    "${\\partial E}/{\\partial {\\bf W}_h} = ({\\bf y}_t - {\\bf y}_p) {\\bf y}_p (1 - {\\bf y}_p)\\bf {h}$\n",
    "\n",
    "where ${\\bf h} = \\sigma({\\bf W}_i {\\bf x}_i + {\\bf b}_i)$\n",
    "\n",
    "In above code:\n",
    "$D = {\\bf y}_t - {\\bf y}_p$\n",
    "\n",
    "${\\bf y}_p (1 - {\\bf y}_p)$ = slope_hidden_layer\n",
    "\n",
    "$\\bf {h}$ = hiddenlayer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Resources:\n",
    "- https://enlight.nyc/projects/neural-network/\n",
    "- WTF is Tensor: https://www.kdnuggets.com/2018/05/wtf-tensor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day3\"></a> \n",
    "# Day 3: Neural Network from Scratch\n",
    "https://docs.google.com/presentation/d/1QstriVSuk8ZXWYjPrJ6SZUTjjYTC9Z6HHwJAun-hBEE/edit#slide=id.p\n",
    "\n",
    "### Interview Questions\n",
    "- What is the bias variance tradeoff in machine learning?\n",
    "    - Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance\n",
    "    - https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/#:~:text=Bias%20is%20the%20simplifying%20assumptions,the%20bias%20and%20the%20variance.\n",
    "\n",
    "- What is an example of a low bias ML algorithm?\n",
    "    - Low Variance: Suggests small changes to the estimate of the target function with changes to the training dataset.\n",
    "        - Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "    - High Variance: Suggests large changes to the estimate of the target function with changes to the training dataset.\n",
    "        - Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.\n",
    "        - Can potentially overfit\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-1.png?raw=true\" width=\"750\">\n",
    "\n",
    "- Bias\n",
    "    - Shifts curve of our activation function\n",
    "    - Weights can help give a guidance towards your model\n",
    "    - Help optimize parameters to help\n",
    "    - Allows us to place significance over selected parameters\n",
    "    - Traditionally set to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Machine Learning for Beginners: An Introduction to Neural Networks\n",
    "https://victorzhou.com/blog/intro-to-neural-networks/\n",
    "\n",
    "## Neuron\n",
    "- A neuron takes inputs, does some math with them, and produces one output\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-2.png?raw=true\" width=\"500\">\n",
    "\n",
    "1. First, each input is multiplied by a weight:\n",
    "\n",
    "    `x1 -> x1 * w1`\n",
    "    `x2 -> x2 * w2`\n",
    "    \n",
    "2. All the weighted inputs are added together with a bias *b*:\n",
    "    \n",
    "    `(x1 * w1) + (x2 * w2) + b`\n",
    "    \n",
    "3. The sum is passed through an activation function:\n",
    "\n",
    "    `y = f(x1*w1 + x2*w2 + b)`\n",
    "\n",
    "### Activation Function\n",
    "- used to turn an unbounded input into an output that has a nice, predictable form. \n",
    "- A commonly used activation function is the sigmoid function\n",
    "\n",
    "## Sigmoid\n",
    "- The sigmoid function only outputs numbers in the range (0,1). You can think of it as compressing (−∞,+∞) to (0,1) - big negative numbers become ~0, and big positive numbers become ~1.\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-3.png?raw=true\" width=\"500\">\n",
    "\n",
    "### Neuron Code in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"This class represents an articfical sigmoid neuron\"\"\"\n",
    "\n",
    "    def __init__(self, weights, bias, activation_function):\n",
    "        \"\"\" initializes this neuron with weights, inputs, and bias\"\"\"\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def feedForward(self, inputs):\n",
    "        \"\"\"steps to return the y output using dot function\"\"\"\n",
    "        result = np.dot(self.weights, inputs) + self.bias\n",
    "        return self.activation_function(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933071490757153\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0,1])\n",
    "bias = 1\n",
    "inputs = np.array([2,4])\n",
    "#instantiate a neuron\n",
    "neuron = Neuron(weights, bias, sigmoid)\n",
    "print(neuron.feedForward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "### The Neural Network's FeedForward\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-4.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "        #inputs or X's\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #hidden layer\n",
    "        self.h1 = Neuron(weights, bias, sigmoid)\n",
    "        self.h2 = Neuron(weights, bias, sigmoid)\n",
    "        \n",
    "        #output layer\n",
    "        self.o1 = Neuron(weights, bias, sigmoid)\n",
    "        \n",
    "    def feedForward(self, inputs):\n",
    "        \"\"\"get hidden layer outputs\"\"\"\n",
    "        out_h1 = self.h1.feedForward(inputs)\n",
    "        out_h2 = self.h2.feedForward(inputs)\n",
    "        \n",
    "        out_o1 = self.o1.feedForward(np.array([out_h1, out_h2]))\n",
    "        return out_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8800925786929503\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0, 1])\n",
    "bias = 1\n",
    "\n",
    "simpleNN = SimpleNeuralNetwork(weights, bias)\n",
    "inputs = np.array([2, 4])\n",
    "\n",
    "print(simpleNN.feedForward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW: \n",
    "Check out this [tutorial](https://victorzhou.com/blog/intro-to-neural-networks/) to prep for the backpropagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList = [1,2,3,4,5]\n",
    "myList[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day4\"></a> \n",
    "# Day 4: Intro to Keras\n",
    "https://docs.google.com/presentation/d/1QstriVSuk8ZXWYjPrJ6SZUTjjYTC9Z6HHwJAun-hBEE/edit#slide=id.p\n",
    "\n",
    "## Back propagation\n",
    "Once we have randomly initialized the weights how do we tune them?\n",
    "- We need to be able to measure how “good” our neural network is doing often referred to as a loss function\n",
    "- Once we find a measure we need to adjust the weights in a way that will minimize the “cost” of this measure\n",
    "- Trying to calculate every combination of weights to see which one reduces the cost the most is not efficient\n",
    "- To minimize the loss function a technique called gradient descent is often used in ML\n",
    "\n",
    "## The Loss Function\n",
    "- *“A loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event”*\n",
    "- There are many different loss functions, the one we will use in the example is called Mean Squared Error (MSE)\n",
    "\n",
    "## Mean Squared Error\n",
    "- n: number of samples     \n",
    "- y pred: output being predicted by our model\n",
    "- y true: actual output from training data \n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day4-1.png?raw=true\" width=\"500\">\n",
    "\n",
    "We square the error and take the average because it makes [the math nicer](https://www.benkuhn.net/squared/)\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "- How loss function minimize the loss or \"train\" our network\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day4-2.png?raw=true\" width=\"750\">\n",
    "\n",
    "## Notes\n",
    "- Bad thing about `train-test-split` we can lose valuable information by not including data in the test\n",
    "    - Not having certain valuable information can make things unpredictable\n",
    "- `Cross validation` validate data by the fold, then the other fold, and then avarage the result to get a balance training and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category vs. Numerical\n",
    "- **Keras takes numerical, not categorical**\n",
    "- Category removes the greater value and evens the playing field for different possible targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_train_one_hot = np_utils.to_categorical(y_train)\n",
    "y_test_one_hot = np_utils.to_categorical(y_test)\n",
    "\n",
    "# print(y_test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning in Keras\n",
    "\n",
    "### Deep Learning Platforms in Python\n",
    "1. Keras \n",
    "2. Tensorflow \n",
    "3. Pytorch \n",
    "4. Caffe \n",
    "5. Theano \n",
    "6. CNTK 7- MXNET\n",
    "\n",
    "### Why we use Keras in DS 2.2 ?\n",
    "- A focus on user experience, easy to build and train a deep learning model\n",
    "- Easy to learn and easy to use\n",
    "- Large adoption in the industry and research community\n",
    "- Multi-backend, multi-platform\n",
    "- Easy productization of models\n",
    "\n",
    "## Keras has two API Styles\n",
    "\n",
    "### 1. Sequential API\n",
    "- Dead simple\n",
    "- **Only for single-input, single-output, sequential layer stacks**\n",
    "- Good for **70+%** of use cases\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day4-3.png?raw=true\" width=\"1000\">\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Functional API\n",
    "- Like playing with Lego bricks\n",
    "- **Multi-input, multi-output, arbitrary static graph topologies**\n",
    "- Good for **95%** of use cases\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day4-4.png?raw=true\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jess's example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 892us/step - loss: 1.1350 - accuracy: 0.2857\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 1000us/step - loss: 1.1085 - accuracy: 0.2857\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 932us/step - loss: 1.0905 - accuracy: 0.3429\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 826us/step - loss: 1.0761 - accuracy: 0.3524\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 857us/step - loss: 1.0617 - accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 911us/step - loss: 1.0461 - accuracy: 0.2286\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 782us/step - loss: 1.0302 - accuracy: 0.4286\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 893us/step - loss: 1.0137 - accuracy: 0.3238\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 834us/step - loss: 0.9959 - accuracy: 0.4476\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 771us/step - loss: 0.9755 - accuracy: 0.4476\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 840us/step - loss: 0.9526 - accuracy: 0.6095\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 829us/step - loss: 0.9255 - accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 717us/step - loss: 0.8956 - accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 810us/step - loss: 0.8644 - accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 768us/step - loss: 0.8299 - accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 867us/step - loss: 0.7957 - accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 837us/step - loss: 0.7620 - accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 791us/step - loss: 0.7313 - accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 838us/step - loss: 0.7014 - accuracy: 0.6667\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 934us/step - loss: 0.6756 - accuracy: 0.7619\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 889us/step - loss: 0.6525 - accuracy: 0.7333\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 893us/step - loss: 0.6319 - accuracy: 0.7333\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 941us/step - loss: 0.6141 - accuracy: 0.7143\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 816us/step - loss: 0.5965 - accuracy: 0.7429\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 787us/step - loss: 0.5839 - accuracy: 0.7619\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 790us/step - loss: 0.5686 - accuracy: 0.8381\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 809us/step - loss: 0.5573 - accuracy: 0.8476\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 956us/step - loss: 0.5469 - accuracy: 0.7333\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 863us/step - loss: 0.5352 - accuracy: 0.8762\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 899us/step - loss: 0.5284 - accuracy: 0.8095\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 786us/step - loss: 0.5155 - accuracy: 0.8762\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 874us/step - loss: 0.5113 - accuracy: 0.8000\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 880us/step - loss: 0.4975 - accuracy: 0.8095\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 871us/step - loss: 0.4942 - accuracy: 0.9238\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 829us/step - loss: 0.4860 - accuracy: 0.9429\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 822us/step - loss: 0.4752 - accuracy: 0.8952\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 774us/step - loss: 0.4700 - accuracy: 0.9238\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 802us/step - loss: 0.4629 - accuracy: 0.9429\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 812us/step - loss: 0.4538 - accuracy: 0.9714\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 908us/step - loss: 0.4452 - accuracy: 0.8952\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 848us/step - loss: 0.4379 - accuracy: 0.9238\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 820us/step - loss: 0.4298 - accuracy: 0.9238\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 815us/step - loss: 0.4180 - accuracy: 0.9524\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 791us/step - loss: 0.4157 - accuracy: 0.9429\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 859us/step - loss: 0.4025 - accuracy: 0.9429\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 813us/step - loss: 0.3963 - accuracy: 0.9619\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3882 - accuracy: 0.9810\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 1000us/step - loss: 0.3822 - accuracy: 0.9619\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.9429\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 869us/step - loss: 0.3695 - accuracy: 0.9524\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 893us/step - loss: 0.3584 - accuracy: 0.9429\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 947us/step - loss: 0.3499 - accuracy: 0.9714\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 897us/step - loss: 0.3431 - accuracy: 0.9619\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 862us/step - loss: 0.3357 - accuracy: 0.9524\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 771us/step - loss: 0.3288 - accuracy: 0.9714\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 883us/step - loss: 0.3233 - accuracy: 0.9524\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 871us/step - loss: 0.3162 - accuracy: 0.9619\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 880us/step - loss: 0.3062 - accuracy: 0.9619\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 709us/step - loss: 0.3042 - accuracy: 0.9714\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 720us/step - loss: 0.2933 - accuracy: 0.9714\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2910 - accuracy: 0.9333\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 890us/step - loss: 0.2829 - accuracy: 0.9524\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 887us/step - loss: 0.2726 - accuracy: 0.9619\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 625us/step - loss: 0.2709 - accuracy: 0.9619\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 607us/step - loss: 0.2633 - accuracy: 0.9714\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 544us/step - loss: 0.2569 - accuracy: 0.9619\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 842us/step - loss: 0.2536 - accuracy: 0.9524\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.2483 - accuracy: 0.9619\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 807us/step - loss: 0.2446 - accuracy: 0.9524\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 712us/step - loss: 0.2350 - accuracy: 0.9524\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 724us/step - loss: 0.2316 - accuracy: 0.9714\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 690us/step - loss: 0.2239 - accuracy: 0.9619\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 695us/step - loss: 0.2229 - accuracy: 0.9714\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 752us/step - loss: 0.2194 - accuracy: 0.9905\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 818us/step - loss: 0.2154 - accuracy: 0.9810\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 770us/step - loss: 0.2121 - accuracy: 0.9524\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 719us/step - loss: 0.2125 - accuracy: 0.9524\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 696us/step - loss: 0.2039 - accuracy: 0.9714\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 648us/step - loss: 0.1975 - accuracy: 0.9810\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 709us/step - loss: 0.1953 - accuracy: 0.9619\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 701us/step - loss: 0.1926 - accuracy: 0.9429\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 633us/step - loss: 0.1888 - accuracy: 0.9810\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 648us/step - loss: 0.1865 - accuracy: 0.9429\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 744us/step - loss: 0.1821 - accuracy: 0.9810\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 789us/step - loss: 0.1776 - accuracy: 0.9810\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 824us/step - loss: 0.1773 - accuracy: 0.9619\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 651us/step - loss: 0.1695 - accuracy: 0.9714\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 686us/step - loss: 0.1716 - accuracy: 0.9810\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 788us/step - loss: 0.1684 - accuracy: 0.9810\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 774us/step - loss: 0.1705 - accuracy: 0.9524\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9619\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 877us/step - loss: 0.1590 - accuracy: 0.9619\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 865us/step - loss: 0.1646 - accuracy: 0.9619\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 804us/step - loss: 0.1612 - accuracy: 0.9619\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 775us/step - loss: 0.1551 - accuracy: 0.9810\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 892us/step - loss: 0.1508 - accuracy: 0.9714\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 764us/step - loss: 0.1468 - accuracy: 0.9619\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 912us/step - loss: 0.1520 - accuracy: 0.9619\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 873us/step - loss: 0.1421 - accuracy: 0.9810\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 851us/step - loss: 0.1406 - accuracy: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x145321400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(4,))) #put its input. What is the output shape of that layer going to be?\n",
    "model.add(Activation('sigmoid'))#specified activation function for those layers\n",
    "model.add(Dense(3)) #lessen the nodes\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=1) #how many time do we descend in our Gradient Decend; batch_size = how many samples considered each time we update our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 898us/step - loss: 0.2084 - accuracy: 0.8889\n",
      "0.8888888955116272\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply NN with Keras on iris data\n",
    "- Use Sequential API for Keras\n",
    "- Use 70 percent of data for train\n",
    "- Use one-hot encoding for labels with from keras.utils import np_utils\n",
    "- Define two layers fully connected network\n",
    "- Define categorical_crossentropy as the loss (cost) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.98\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "y_train_one_hot = np_utils.to_categorical(y_train)\n",
    "y_test_one_hot = np_utils.to_categorical(y_test)\n",
    "\n",
    "# print(y_one_hot)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(4,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=1, verbose=0);\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply NN with Keras on iris data with Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.98\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "y_train_one_hot = np_utils.to_categorical(y_train)\n",
    "y_test_one_hot = np_utils.to_categorical(y_test)\n",
    "\n",
    "# print(y_one_hot)\n",
    "\n",
    "inp = Input(shape=(4,))\n",
    "x = Dense(16, activation='sigmoid')(inp)\n",
    "out = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs= out)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=1, verbose=0);\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day5\"></a> \n",
    "# Day 5: Backward Propagation\n",
    "https://docs.google.com/presentation/d/1QstriVSuk8ZXWYjPrJ6SZUTjjYTC9Z6HHwJAun-hBEE/edit#slide=id.g9fd5bc0407_0_5\n",
    "\n",
    "## Back Propagation\n",
    "- Once we have randomly initialized the weights how do we tune them?\n",
    "- We need to be able to **measure how “good” our neural network** is doing often referred to as a **loss function**\n",
    "- Once we find a measure we **need to adjust the weights in a way that will minimize the “cost”** of this measure\n",
    "- Trying to calculate every combination of weights to see which one reduces the cost the most is not efficient\n",
    "- To **minimize the loss function a technique called gradient descent** is often used in ML\n",
    "\n",
    "## The Loss Function\n",
    "- “A loss function or cost function is a **function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event”**\n",
    "- There are many different loss functions, the one we will use in the example is called **Mean Squared Error (MSE)**\n",
    "\n",
    "## Back Propagation\n",
    "- We are going to work through [this article](https://towardsdatascience.com/backpropagation-for-people-who-are-afraid-of-math-936a2cbebed7)\n",
    "\n",
    "#### Backpropagation Steps:\n",
    "1. Calculate the cost function, C(w)\n",
    "2. Calculate the gradient of C(w) with respect to (w.r.t) all the weights, w, and biases, b, in your neural network (NN)\n",
    "3. Adjust the w and b proportionally to their gradients\n",
    "Also check out [this article](https://towardsdatascience.com/the-maths-behind-back-propagation-cf6714736abf) for more math \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "  \"\"\"This class represents an articfical sigmoid neuron\"\"\"\n",
    "\n",
    "  def __init__(self, weights, bias, activation_function):\n",
    "    \"\"\" initializes this neuron with weights, inputs, and bias\"\"\"\n",
    "\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "    self.activation_function = activation_function\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    #TODO: code the steps to return the y output\n",
    "    result = np.dot(self.weights, inputs) + self.bias\n",
    "    return self.activation_function(result)\n",
    "\n",
    "weights = np.array([0,1])\n",
    "bias = 1\n",
    "inputs = np.array([2,4])\n",
    "#instantiate a neuron\n",
    "neuron = Neuron(weights, bias, sigmoid)\n",
    "#print(neuron.feedforward(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8800925786929503\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "    #hidden layer\n",
    "    self.h1 = Neuron(weights, bias, sigmoid)\n",
    "    self.h2 = Neuron(weights, bias, sigmoid)\n",
    "\n",
    "    #output layer\n",
    "    self.o1 = Neuron(weights, bias, sigmoid)\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    #get hidden layer outputs\n",
    "    out_h1 = self.h1.feedforward(inputs)\n",
    "    out_h2 = self.h2.feedforward(inputs)\n",
    "\n",
    "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "    return out_o1\n",
    "\n",
    "#y_pred and y_true are numpy arrays\n",
    "def mse_loss(y_true, y_pred):\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "  \n",
    "weights = np.array([0,1])\n",
    "bias = 1\n",
    "simplenn = SimpleNeuralNetwork(weights, bias)\n",
    "inputs = np.array([2,4])\n",
    "print(simplenn.feedforward(inputs))\n",
    "\n",
    "y_true = np.array([1,0,0,1])\n",
    "y_pred = np.array([0,0,0,0])\n",
    "print(mse_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives Rules\n",
    "https://www.mathsisfun.com/calculus/derivatives-rules.html\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day5-1.png?raw=true\" width=\"500\">\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day5-2.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day5-3.png?raw=true\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Notebooks\n",
    "- [Deep Learning Glossary](https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Lessons/DeepLearningGlossary.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day8\"></a> \n",
    "# Day 8: Convolutional Neural Network\n",
    "- **Sequential** is a NN model that allows us to add layer by building it step by step\n",
    "- **Dense** adds a layer of neurons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"lab1\"></a> \n",
    "### Lab 1\n",
    "\n",
    "Tutorial: [Python Convolutional Neural Network: Creating a CNN in Keras, TensorFlow and Plain Python](https://missinglink.ai/guides/convolutional-neural-networks/python-convolutional-neural-network-creating-cnn-keras-tensorflow-plain-python/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 73s 39ms/step - loss: 0.2210 - accuracy: 0.9521 - val_loss: 0.0966 - val_accuracy: 0.9716\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 78s 42ms/step - loss: 0.0660 - accuracy: 0.9801 - val_loss: 0.0848 - val_accuracy: 0.9750\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 81s 43ms/step - loss: 0.0479 - accuracy: 0.9848 - val_loss: 0.0758 - val_accuracy: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.77071724e-14, 4.02404522e-16, 1.33951239e-10, 2.40395437e-08,\n",
       "        6.78720036e-16, 3.99273408e-14, 1.53550912e-18, 1.00000000e+00,\n",
       "        1.03526963e-12, 9.96556726e-10],\n",
       "       [8.09913914e-09, 3.13066617e-09, 9.99999762e-01, 4.81141849e-10,\n",
       "        3.86484396e-14, 2.07901814e-14, 2.24898642e-08, 3.40396873e-13,\n",
       "        2.81714961e-07, 2.78229761e-15],\n",
       "       [4.30938529e-09, 9.99942541e-01, 1.08588370e-06, 4.05593141e-08,\n",
       "        4.64521044e-08, 4.60761065e-08, 3.15033262e-08, 1.80602513e-07,\n",
       "        5.60371482e-05, 5.86901416e-09],\n",
       "       [9.99990940e-01, 2.75990428e-12, 1.09197366e-07, 7.00521509e-13,\n",
       "        2.02911611e-11, 9.87869253e-09, 3.80378606e-06, 6.28115338e-11,\n",
       "        1.14980672e-07, 5.02158673e-06]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.layers import Conv2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "#TODO: get train and test data\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#add convolutional layer\n",
    "filters = 64\n",
    "kernel_size = 3 #3x3 matrix\n",
    "activation_function = \"relu\" #rectified linear activation function\n",
    "model.add(Conv2D(filters, kernel_size=kernel_size, activation=activation_function, input_shape=(28,28,1)))\n",
    "\n",
    "#TODO: might add some other Dense layers here and Flatten()\n",
    "model.add(Conv2D(32, kernel_size=3, activation=activation_function))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "#compile the model by setting the optimizer, loss function and metrics\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#fit to train and test\n",
    "# model.fit(X_train, y_train, epochs=3)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)\n",
    "\n",
    "#predict\n",
    "# model.predict(X_test)\n",
    "model.predict(X_test[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milad's Convolutional Neural Network Notebook\n",
    "- [Source](https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Lessons/ConvolutionalNeuralNetwork.md)\n",
    "\n",
    "### Learning Objectives\n",
    "- we learn what are Convolutional Neural Network (CNN)\n",
    "- Will talk about CNN components such as stride, max or average pooling\n",
    "- Discuss how we can obtain the parameters for CNN\n",
    "\n",
    "### Convolutional Neural Network (CNN)\n",
    "- CNN is basically two dimensional configuration of neural networks\n",
    "- The input of CNN are image (three N by N if it color image and N by N if its black and white image)\n",
    "- The weights are also two dimensional array\n",
    "\n",
    "<img src=\"https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Notebooks/Images/rgb_image.png?raw=true\" width=\"500\">\n",
    "\n",
    "### The weights in CNN\n",
    "The weights in CNN are called:\n",
    "- Kernel\n",
    "or\n",
    "- Filter matrix\n",
    "<img src=\"https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Notebooks/Images/kernel_image.png?raw=true\" width=\"600\">\n",
    "\n",
    "### Stride\n",
    "- To define a CNN, we should specify the horizontal and vertical movement steps\n",
    "- what is the output size with stride = 1 and stride =2?\n",
    "<img src=\"https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Notebooks/Images/stride_1.png?raw=true\" width=\"700\">\n",
    "<img src=\"https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Notebooks/Images/stride_2.png?raw=true\" width=\"700\">\n",
    "\n",
    "- output_size = (input_size - filter_size)/stride + 1\n",
    "- Stride visualization: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution\n",
    "\n",
    "`model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=input_shape))`\n",
    "\n",
    "### Max Pooling vs Average Pooling\n",
    "- Max pooling: take the maximum element from each window of a certain size\n",
    "<img src=\"https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Notebooks/Images/maxpooling.png?raw=true\" width=\"700\">\n",
    "\n",
    "### Flatten Layer\n",
    "- After feature extraction that is done by multiple Convolutional layers, we use flatten layer to add MLP after convolutional layers in order to do classification task\n",
    "- This one is simple--it's just Keras's version of numpy.reshape. This reshapes n-dimensional arrays to a vector. This is necessary when moving from Conv2D layers, which expect 2-dimensional arrays as inputs, to Dense layers, which expect 1-dimension vectors as inputs. As a concrete example, a Flatten layer given a 28 x 28 array as input would output a vector of the shape (784, 1)\n",
    "\n",
    "### Visualize the whole NN: CNN + MLP\n",
    "<img src=\"https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Notebooks/Images/CNN.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Obtain the number of parameters for the following CNN\n",
    "By default, the strides = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "output_size = (28 - 3)/1 + 1 = 26\n",
    "\n",
    "output_size = (26 - 3)/1 + 1 = 24\n",
    "\n",
    "The parameters for the first Conv2D = 32 x 9 + 32 = 320\n",
    "\n",
    "The parameters for the second Conv2D = 64 x 32 x 9 + 64 = 18496\n",
    "\n",
    "The shape for flatten is: 12 x 12 x 64 = 9216\n",
    "\n",
    "The parameters for dense_1 = 9216 x 128 + 128 = 1179776\n",
    "\n",
    "The parameters for dense_2 = 128 x 10 + 10 = 1290\n",
    "\n",
    "### Data Preparation for CNN\n",
    "Suppose we want to feed a 4 by 4 image to a CNN network, how we should reshape the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 4.181149  17.891964   0.       ]\n",
      "   [ 5.3029346 15.295205   0.       ]]\n",
      "\n",
      "  [[ 3.5036564 18.106941   0.       ]\n",
      "   [ 4.926444  15.246883   0.       ]]]]\n",
      "M :\n",
      "[[[ 4.181149  17.891964 ]\n",
      "  [ 0.         5.3029346]]\n",
      "\n",
      " [[15.295205   0.       ]\n",
      "  [ 3.5036564 18.106941 ]]\n",
      "\n",
      " [[ 0.         4.926444 ]\n",
      "  [15.246883   0.       ]]]\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4, 4, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 3, 3, 2)           10        \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 2, 2, 3)           27        \n",
      "=================================================================\n",
      "Total params: 37\n",
      "Trainable params: 37\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(28, 28, 1)\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "input_img = Input(shape=(4, 4, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(2, (2, 2), activation='relu')(input_img)\n",
    "y = Conv2D(3, (2, 2), activation='relu')(x)\n",
    "model = Model(input_img, y)\n",
    "# cnv_ml_1 = Model(input_img, x)\n",
    "\n",
    "data = np.array([[5, 12, 1, 8], [2, 10, 3, 6], [4, 7, 9, 1], [5, 7, 5, 6]])\n",
    "data = data.reshape(1, 4, 4, 1)\n",
    "print(model.predict(data))\n",
    "print('M :')\n",
    "print(model.predict(data).reshape(3, 2, 2))\n",
    "print(model.summary())\n",
    "from keras.datasets import mnist\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print(x_train[0].shape)\n",
    "print(x_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day10\"></a> \n",
    "# Day 10: Recurrent Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

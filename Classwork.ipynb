{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 2.2 Deep Learning: \n",
    "- **Make School Courses repo**: click [here](https://github.com/Make-School-Courses/DS-2.2-Deep-Learning)\n",
    "- **My GitHub Repo**: click [here](https://github.com/SamuelFolledo/DS2.2-Deep-Learning)\n",
    "\n",
    "\n",
    "**Disclaimer:** Table of Contents only works on Jupyter Notebook\n",
    "\n",
    "## Table of Contents\n",
    "1. **[Introduction to Deep Learning](#day1)**\n",
    "2. **[What is Neural Network](#day2)**\n",
    "3. **[Neural Network from Scratch](#day3)**\n",
    "4. **[Intro To Keras](#day4)**\n",
    "5. **[](#day5)**\n",
    "6. **[](#day6)**\n",
    "7. **[](#day7)**\n",
    "8. **[](#day8)**\n",
    "9. **[](#day9)**\n",
    "10. **[](#day10)**\n",
    "11. **[](#day11)**\n",
    "12. **[](#day12)**\n",
    "13. **[](#day13)**\n",
    "14. **[](#day14)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day1\"></a> \n",
    "# Day 1: Introduction to Deep Learning\n",
    "https://docs.google.com/presentation/d/1vxQ_R8-gEtR896EoC1_ObSKoWJZje40H8wcXzdltjpo/edit#slide=id.g9e8c78a5a9_0_18\n",
    "\n",
    "### What We’re Going to Learn\n",
    "- High Level Overview of Deep Learning\n",
    "- Applications of Deep Learning \n",
    "- What is a Neural Network\n",
    "\n",
    "\n",
    "## Deep Learning\n",
    "- Deep learning is a **subfield of machine learning**\n",
    "- Deep learning is **all about neural networks**\n",
    "- Deep learning **refers to large neural networks**\n",
    "- Neural networks have been around for a while but is only relatively recently that we have the computing power to train and run the these larger more complex neural networks\n",
    "- The deep part comes from **having many layers in the networks**\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-1.png?raw=true\" width=\"700\">\n",
    "\n",
    "### Why Deep Learning?\n",
    "*“The analogy to deep learning is that the rocket engine is the deep learning models and the fuel is the huge amounts of data we can feed to these algorithms.”* - Andrew Ng\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-2.png?raw=true\" width=\"500\">\n",
    "\n",
    "### Drawbacks of Deep Learning\n",
    "- Interpretability is harder, very black box \n",
    "- Doesn’t work well with small datasets\n",
    "    - the less data you have, the worse it's going to perform\n",
    "    - more instances/samples (~10,000) not how many features\n",
    "- Needs lots of computing power\n",
    "\n",
    "## Neural Networks\n",
    "- Modeled loosely on the human brain\n",
    "- Composed of many simple processing nodes or “neurons”\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-3.png?raw=true\" width=\"700\">\n",
    "\n",
    "## A Neuron\n",
    "- For most modern deep learning algorithms we use what is called a “sigmoid neuron”\n",
    "- In order to understand these better let’s take a look at the idea they were based on, a perceptron, one of the first types of artificial neurons\n",
    "\n",
    "## Perceptrons\n",
    "- A perceptron takes in multiple inputs and produces a single binary output\n",
    "- Each of these inputs x have a weight w, what would weights do?\n",
    "\t◦ Weights are used to add importance of a feature, giving it more influence on the output\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-4.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "## Deep Neural Networks\n",
    "We add all those x*w’s together to get a sum\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-5.png?raw=true\" width=\"500\">\n",
    "\n",
    "\n",
    "## Activation Function //58m\n",
    "How do we know whether to give a 0 or 1, that’s the activation functions job! One example is the Unit Step Activation Function\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-6.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "### What are some drawbacks of this decision making model? //1h1m\n",
    "- Setting an absolute threshold is very not a good decision￼\n",
    "\n",
    "\n",
    "## Sigmoid Neuron: Much Less Harsh //1h4m\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-7.png?raw=true\" width=\"700\">\n",
    "\n",
    "## Sigmoid Neuron Output //1h5\n",
    "\n",
    "- Our output is no longer 0 or 1\n",
    "- It is now a value from 0 - 1 and can be interpreted like a probability\n",
    "- We now have more control over how that output is interpreted\n",
    "\n",
    "## Putting it all together\n",
    "- A neural network is really just a bunch of interconnected neurons, the output from one layer or neurons can then be used as one the inputs for the next layer\n",
    "- Through a series of layers different patterns of neurons are “activated” producing different end results for different inputs\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-8.png?raw=true\" width=\"700\">\n",
    "\n",
    "## Next Steps\n",
    "- Math happening behind the scenes?\n",
    "- What about those algorithms?\n",
    "- How does it actually get the inputs (features)?\n",
    "- What sorts of parameters and choices can we mess with, how do we optimize?\n",
    "- How do we choose how many layers and neurons?\n",
    "- What tools are available to us?\n",
    "- How can we apply this to a real world problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day2\"></a> \n",
    "# Day 2 What is Neural Network?\n",
    "https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Lessons/WhatisNeuralNetwork.md\n",
    "\n",
    "### Learning Objectives\n",
    "\t•\tWhat is Neural network\n",
    "\t•\tWe learn Forward and backward propagation in NN\n",
    "\t•\tWe implement a NN entirely in numpy\n",
    "\n",
    "## What is Neural Network?\n",
    "It is a **computational system inspired by the Structure, Processing Method and Learning Ability similar to our biological brain**\n",
    "\n",
    "### Characteristics of Artificial Neural Networks\n",
    "\t•\tA large number of very simple processing neuron-like processing elements\n",
    "\t•\tA large number of weighted connections between the elements\n",
    "\t•\tDistributed representation of knowledge over the connections\n",
    "\t•\tKnowledge is acquired by network through a learning process\n",
    "\n",
    "## What is perceptron?\n",
    "\t•\tA perceptron can be understood as anything that **takes multiple inputs and produces one output**\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day2-1.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "## Multi-layer perceptron (MLP)\n",
    "\t•\tMLP is the stack of perceptrons\n",
    "    \n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day2-2.png?raw=true\" width=\"700\">\n",
    "\n",
    "\t•\tIn this image, the yellow nodes are inputs, the blue nodes (at each vertical) are hidden layers and the orange ones are output of the MLP\n",
    "\n",
    "## Forward and backward propagation\n",
    "NN **takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer.** This result estimation process is technically known as **“Forward Propagation“**\n",
    "\n",
    "Next, we **compare the result with actual output**. The task is to make the output to neural network as close to actual (desired) output. This defines our cost function.\n",
    "We try to **obtain the weight of neurons such that the NN total error (our cost function) being minimized**. This process is known as **“Backward Propagation“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#creating a vector\n",
    "vector = np.array([5,2,1])\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98231977]\n",
      " [0.97394954]\n",
      " [0.03823383]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/\n",
    "# Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "\n",
    "#Sigmoid Function //gives us a value between 0 and 1 or Probability\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "\n",
    "#Variable initialization\n",
    "epoch=5000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "    D = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = D * slope_output_layer\n",
    "    Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *lr\n",
    "    bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "    wh += X.T.dot(d_hiddenlayer) *lr\n",
    "    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we update the weights to minimize the error?\n",
    "1. First we should define the cost function. for our example here the MSE is our cost function:\n",
    "\n",
    "$E= \\frac{1}{2} ({\\bf y}_t - {\\bf y}_p)^T ({\\bf y}_t - {\\bf y}_p)$\n",
    "\n",
    "2. We update the weight (${\\bf W}_i$ and ${\\bf W}_h$) such that the error, $E$, being minimized. The most popular algorithm is Gradient Descent:\n",
    "\n",
    "${\\bf W}_h = {\\bf W}_h + \\eta {\\partial E}/{\\partial {\\bf W}_h} $\n",
    "\n",
    "For our above example we can show that:\n",
    "\n",
    "${\\partial E}/{\\partial {\\bf W}_h} = ({\\bf y}_t - {\\bf y}_p) {\\bf y}_p (1 - {\\bf y}_p)\\bf {h}$\n",
    "\n",
    "where ${\\bf h} = \\sigma({\\bf W}_i {\\bf x}_i + {\\bf b}_i)$\n",
    "\n",
    "In above code:\n",
    "$D = {\\bf y}_t - {\\bf y}_p$\n",
    "\n",
    "${\\bf y}_p (1 - {\\bf y}_p)$ = slope_hidden_layer\n",
    "\n",
    "$\\bf {h}$ = hiddenlayer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Resources:\n",
    "- https://enlight.nyc/projects/neural-network/\n",
    "- WTF is Tensor: https://www.kdnuggets.com/2018/05/wtf-tensor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day3\"></a> \n",
    "# Day 3: Neural Network from Scratch\n",
    "https://docs.google.com/presentation/d/1QstriVSuk8ZXWYjPrJ6SZUTjjYTC9Z6HHwJAun-hBEE/edit#slide=id.p\n",
    "\n",
    "### Interview Questions\n",
    "- What is the bias variance tradeoff in machine learning?\n",
    "    - Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance\n",
    "    - https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/#:~:text=Bias%20is%20the%20simplifying%20assumptions,the%20bias%20and%20the%20variance.\n",
    "\n",
    "- What is an example of a low bias ML algorithm?\n",
    "    - Low Variance: Suggests small changes to the estimate of the target function with changes to the training dataset.\n",
    "        - Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "    - High Variance: Suggests large changes to the estimate of the target function with changes to the training dataset.\n",
    "        - Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.\n",
    "        - Can potentially overfit\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-1.png?raw=true\" width=\"750\">\n",
    "\n",
    "- Bias\n",
    "    - Shifts curve of our activation function\n",
    "    - Weights can help give a guidance towards your model\n",
    "    - Help optimize parameters to help\n",
    "    - Allows us to place significance over selected parameters\n",
    "    - Traditionally set to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Machine Learning for Beginners: An Introduction to Neural Networks\n",
    "https://victorzhou.com/blog/intro-to-neural-networks/\n",
    "\n",
    "## Neuron\n",
    "- A neuron takes inputs, does some math with them, and produces one output\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-2.png?raw=true\" width=\"500\">\n",
    "\n",
    "1. First, each input is multiplied by a weight:\n",
    "\n",
    "    `x1 -> x1 * w1`\n",
    "    `x2 -> x2 * w2`\n",
    "    \n",
    "2. All the weighted inputs are added together with a bias *b*:\n",
    "    \n",
    "    `(x1 * w1) + (x2 * w2) + b`\n",
    "    \n",
    "3. The sum is passed through an activation function:\n",
    "\n",
    "    `y = f(x1*w1 + x2*w2 + b)`\n",
    "\n",
    "### Activation Function\n",
    "- used to turn an unbounded input into an output that has a nice, predictable form. \n",
    "- A commonly used activation function is the sigmoid function\n",
    "\n",
    "## Sigmoid\n",
    "- The sigmoid function only outputs numbers in the range (0,1). You can think of it as compressing (−∞,+∞) to (0,1) - big negative numbers become ~0, and big positive numbers become ~1.\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-3.png?raw=true\" width=\"500\">\n",
    "\n",
    "### Neuron Code in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"This class represents an articfical sigmoid neuron\"\"\"\n",
    "\n",
    "    def __init__(self, weights, bias, activation_function):\n",
    "        \"\"\" initializes this neuron with weights, inputs, and bias\"\"\"\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def feedForward(self, inputs):\n",
    "        \"\"\"steps to return the y output using dot function\"\"\"\n",
    "        result = np.dot(self.weights, inputs) + self.bias\n",
    "        return self.activation_function(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933071490757153\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0,1])\n",
    "bias = 1\n",
    "inputs = np.array([2,4])\n",
    "#instantiate a neuron\n",
    "neuron = Neuron(weights, bias, sigmoid)\n",
    "print(neuron.feedForward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "### The Neural Network's FeedForward\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-4.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "        #inputs or X's\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #hidden layer\n",
    "        self.h1 = Neuron(weights, bias, sigmoid)\n",
    "        self.h2 = Neuron(weights, bias, sigmoid)\n",
    "        \n",
    "        #output layer\n",
    "        self.o1 = Neuron(weights, bias, sigmoid)\n",
    "        \n",
    "    def feedForward(self, inputs):\n",
    "        \"\"\"get hidden layer outputs\"\"\"\n",
    "        out_h1 = self.h1.feedForward(inputs)\n",
    "        out_h2 = self.h2.feedForward(inputs)\n",
    "        \n",
    "        out_o1 = self.o1.feedForward(np.array([out_h1, out_h2]))\n",
    "        return out_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8800925786929503\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0, 1])\n",
    "bias = 1\n",
    "\n",
    "simpleNN = SimpleNeuralNetwork(weights, bias)\n",
    "inputs = np.array([2, 4])\n",
    "\n",
    "print(simpleNN.feedForward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW: \n",
    "Check out this [tutorial](https://victorzhou.com/blog/intro-to-neural-networks/) to prep for the backpropagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList = [1,2,3,4,5]\n",
    "myList[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day4\"></a> \n",
    "# Day 4: Intro to Keras\n",
    "https://docs.google.com/presentation/d/1QstriVSuk8ZXWYjPrJ6SZUTjjYTC9Z6HHwJAun-hBEE/edit#slide=id.p\n",
    "\n",
    "## Back propagation\n",
    "Once we have randomly initialized the weights how do we tune them?\n",
    "- We need to be able to measure how “good” our neural network is doing often referred to as a loss function\n",
    "- Once we find a measure we need to adjust the weights in a way that will minimize the “cost” of this measure\n",
    "- Trying to calculate every combination of weights to see which one reduces the cost the most is not efficient\n",
    "- To minimize the loss function a technique called gradient descent is often used in ML\n",
    "\n",
    "## The Loss Function\n",
    "- *“A loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event”*\n",
    "- There are many different loss functions, the one we will use in the example is called Mean Squared Error (MSE)\n",
    "\n",
    "## Mean Squared Error\n",
    "- n: number of samples     \n",
    "- y pred: output being predicted by our model\n",
    "- y true: actual output from training data \n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day4-1.png?raw=true\" width=\"500\">\n",
    "\n",
    "We square the error and take the average because it makes [the math nicer](https://www.benkuhn.net/squared/)\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "- How loss function minimize the loss or \"train\" our network\n",
    "\n",
    "\n",
    "## Notes\n",
    "- Bad thing about `train-test-split` we can lose valuable information by not including data in the test\n",
    "    - Not having certain valuable information can make things unpredictable\n",
    "- `Cross validation` validate data by the fold, then the other fold, and then avarage the result to get a balance training and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category vs. Numerical\n",
    "- **Keras takes numerical, not categorical**\n",
    "- Category removes the greater value and evens the playing field for different possible targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "y_train_one_hot = np_utils.to_categorical(y_train)\n",
    "y_test_one_hot = np_utils.to_categorical(y_test)\n",
    "\n",
    "# print(y_test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning in Keras\n",
    "\n",
    "### Deep Learning Platforms in Python\n",
    "1. Keras \n",
    "2. Tensorflow \n",
    "3. Pytorch \n",
    "4. Caffe \n",
    "5. Theano \n",
    "6. CNTK 7- MXNET\n",
    "\n",
    "### Why we use Keras in DS 2.2 ?\n",
    "- A focus on user experience, easy to build and train a deep learning model\n",
    "- Easy to learn and easy to use\n",
    "- Large adoption in the industry and research community\n",
    "- Multi-backend, multi-platform\n",
    "- Easy productization of models\n",
    "\n",
    "Keras has two API Styles\n",
    "\n",
    "The Sequential API\n",
    "\t•\tDead simple\n",
    "\t•\tOnly for single-input, single-output, sequential layer stacks\n",
    "\t•\tGood for 70+% of use cases\n",
    "￼\n",
    "\n",
    "The functional API\n",
    "\t•\tLike playing with Lego bricks\n",
    "\t•\tMulti-input, multi-output, arbitrary static graph topologies\n",
    "\t•\tGood for 95% of use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jess's example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 686us/step - loss: 1.1061 - accuracy: 0.4952\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 794us/step - loss: 1.0489 - accuracy: 0.6762\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 835us/step - loss: 1.0101 - accuracy: 0.6762\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 823us/step - loss: 0.9744 - accuracy: 0.6286\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 770us/step - loss: 0.9359 - accuracy: 0.6095\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 846us/step - loss: 0.8961 - accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 768us/step - loss: 0.8523 - accuracy: 0.6762\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 838us/step - loss: 0.8081 - accuracy: 0.6381\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 796us/step - loss: 0.7677 - accuracy: 0.6762\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 798us/step - loss: 0.7285 - accuracy: 0.6952\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 815us/step - loss: 0.6939 - accuracy: 0.7048\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 755us/step - loss: 0.6627 - accuracy: 0.6762\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 764us/step - loss: 0.6384 - accuracy: 0.7429\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 762us/step - loss: 0.6108 - accuracy: 0.7143\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 801us/step - loss: 0.5951 - accuracy: 0.7048\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 779us/step - loss: 0.5778 - accuracy: 0.7810\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 819us/step - loss: 0.5614 - accuracy: 0.7810\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 845us/step - loss: 0.5482 - accuracy: 0.8190\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 859us/step - loss: 0.5346 - accuracy: 0.8190\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 853us/step - loss: 0.5230 - accuracy: 0.8286\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 834us/step - loss: 0.5131 - accuracy: 0.8286\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 839us/step - loss: 0.5018 - accuracy: 0.7905\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 790us/step - loss: 0.4944 - accuracy: 0.8381\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 841us/step - loss: 0.4836 - accuracy: 0.8476\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 826us/step - loss: 0.4753 - accuracy: 0.8476\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 721us/step - loss: 0.4704 - accuracy: 0.9048\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 794us/step - loss: 0.4587 - accuracy: 0.9238\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 839us/step - loss: 0.4544 - accuracy: 0.8857\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 803us/step - loss: 0.4450 - accuracy: 0.9143\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 786us/step - loss: 0.4393 - accuracy: 0.9238\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 835us/step - loss: 0.4303 - accuracy: 0.9524\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 784us/step - loss: 0.4192 - accuracy: 0.8952\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 809us/step - loss: 0.4122 - accuracy: 0.9238\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 845us/step - loss: 0.4090 - accuracy: 0.9238\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 812us/step - loss: 0.3977 - accuracy: 0.9429\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 836us/step - loss: 0.3887 - accuracy: 0.9429\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 896us/step - loss: 0.3834 - accuracy: 0.9238\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 909us/step - loss: 0.3748 - accuracy: 0.9524\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 903us/step - loss: 0.3718 - accuracy: 0.9429\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.9429\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 912us/step - loss: 0.3526 - accuracy: 0.9429\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 864us/step - loss: 0.3492 - accuracy: 0.9714\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 904us/step - loss: 0.3343 - accuracy: 0.9238\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 824us/step - loss: 0.3303 - accuracy: 0.9238\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 802us/step - loss: 0.3307 - accuracy: 0.9524\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 784us/step - loss: 0.3160 - accuracy: 0.9524\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 744us/step - loss: 0.3154 - accuracy: 0.9238\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 788us/step - loss: 0.3086 - accuracy: 0.9524\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 750us/step - loss: 0.3085 - accuracy: 0.9238\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 743us/step - loss: 0.2983 - accuracy: 0.9238\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 722us/step - loss: 0.2958 - accuracy: 0.9429\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 708us/step - loss: 0.2898 - accuracy: 0.9333\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 787us/step - loss: 0.2807 - accuracy: 0.9619\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 739us/step - loss: 0.2802 - accuracy: 0.9619\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 669us/step - loss: 0.2753 - accuracy: 0.9238\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 570us/step - loss: 0.2691 - accuracy: 0.9619\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 559us/step - loss: 0.2623 - accuracy: 0.9619\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 599us/step - loss: 0.2622 - accuracy: 0.9333\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 608us/step - loss: 0.2521 - accuracy: 0.9524\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 650us/step - loss: 0.2548 - accuracy: 0.9619\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 636us/step - loss: 0.2370 - accuracy: 0.9524\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 594us/step - loss: 0.2459 - accuracy: 0.9619\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 606us/step - loss: 0.2398 - accuracy: 0.9524\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 641us/step - loss: 0.2344 - accuracy: 0.9524\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 612us/step - loss: 0.2241 - accuracy: 0.9619\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 648us/step - loss: 0.2218 - accuracy: 0.9810\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 608us/step - loss: 0.2232 - accuracy: 0.9524\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 619us/step - loss: 0.2226 - accuracy: 0.9524\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 642us/step - loss: 0.2181 - accuracy: 0.9524\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 591us/step - loss: 0.2181 - accuracy: 0.9714\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 605us/step - loss: 0.2101 - accuracy: 0.9524\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 582us/step - loss: 0.2061 - accuracy: 0.9524\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 595us/step - loss: 0.2056 - accuracy: 0.9524\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 693us/step - loss: 0.2081 - accuracy: 0.9524\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 798us/step - loss: 0.1971 - accuracy: 0.9524\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 761us/step - loss: 0.2031 - accuracy: 0.9429\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 770us/step - loss: 0.1894 - accuracy: 0.9714\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 742us/step - loss: 0.1966 - accuracy: 0.9619\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 707us/step - loss: 0.1936 - accuracy: 0.9429\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 863us/step - loss: 0.1807 - accuracy: 0.9714\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 826us/step - loss: 0.1868 - accuracy: 0.9429\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 819us/step - loss: 0.1847 - accuracy: 0.9524\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 721us/step - loss: 0.1845 - accuracy: 0.9429\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 796us/step - loss: 0.1812 - accuracy: 0.9524\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 924us/step - loss: 0.1796 - accuracy: 0.9619\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 916us/step - loss: 0.1749 - accuracy: 0.9619\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 775us/step - loss: 0.1731 - accuracy: 0.9429\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 707us/step - loss: 0.1767 - accuracy: 0.9619\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 622us/step - loss: 0.1580 - accuracy: 0.9619\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 709us/step - loss: 0.1761 - accuracy: 0.9619\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 687us/step - loss: 0.1733 - accuracy: 0.9524\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 827us/step - loss: 0.1721 - accuracy: 0.9619\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 740us/step - loss: 0.1669 - accuracy: 0.9810\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 733us/step - loss: 0.1691 - accuracy: 0.9524\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 754us/step - loss: 0.1677 - accuracy: 0.9619\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1608 - accuracy: 0.9714\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 808us/step - loss: 0.1600 - accuracy: 0.9429\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 815us/step - loss: 0.1621 - accuracy: 0.9333\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 827us/step - loss: 0.1572 - accuracy: 0.9619\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 802us/step - loss: 0.1602 - accuracy: 0.9524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x144f0b2b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(4,))) #put its input. What is the output shape of that layer going to be?\n",
    "model.add(Activation('sigmoid'))#specified activation function for those layers\n",
    "model.add(Dense(3)) #lessen the nodes\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=1) #how many time do we descend in our Gradient Decend; batch_size = how many samples considered each time we update our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 1.0000\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply NN with Keras on iris data\n",
    "- Use Sequential API for Keras\n",
    "- Use 70 percent of data for train\n",
    "- Use one-hot encoding for labels with from keras.utils import np_utils\n",
    "- Define two layers fully connected network\n",
    "- Define categorical_crossentropy as the loss (cost) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_test_function.<locals>.test_function at 0x144092dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy = 0.98\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "y_train_one_hot = np_utils.to_categorical(y_train)\n",
    "y_test_one_hot = np_utils.to_categorical(y_test)\n",
    "\n",
    "# print(y_one_hot)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(4,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=1, verbose=0);\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply NN with Keras on iris data with Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 19 calls to <function Model.make_test_function.<locals>.test_function at 0x14497f8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy = 0.98\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "y_train_one_hot = np_utils.to_categorical(y_train)\n",
    "y_test_one_hot = np_utils.to_categorical(y_test)\n",
    "\n",
    "# print(y_one_hot)\n",
    "\n",
    "inp = Input(shape=(4,))\n",
    "x = Dense(16, activation='sigmoid')(inp)\n",
    "out = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs= out)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=1, verbose=0);\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

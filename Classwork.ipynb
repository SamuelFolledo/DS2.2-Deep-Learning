{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 2.2 Deep Learning: \n",
    "- **Make School Courses repo**: click [here](https://github.com/Make-School-Courses/DS-2.2-Deep-Learning)\n",
    "- **My GitHub Repo**: click [here](https://github.com/SamuelFolledo/DS2.2-Deep-Learning)\n",
    "\n",
    "\n",
    "**Disclaimer:** Table of Contents only works on Jupyter Notebook\n",
    "\n",
    "## Table of Contents\n",
    "1. **[Introduction to Deep Learning](#day1)**\n",
    "2. **[What is Neural Network](#day2)**\n",
    "3. **[Neural Network from Scratch](#day3)**\n",
    "4. **[](#day4)**\n",
    "5. **[](#day5)**\n",
    "6. **[](#day6)**\n",
    "7. **[](#day7)**\n",
    "8. **[](#day8)**\n",
    "9. **[](#day9)**\n",
    "10. **[](#day10)**\n",
    "11. **[](#day11)**\n",
    "12. **[](#day12)**\n",
    "13. **[](#day13)**\n",
    "14. **[](#day14)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day1\"></a> \n",
    "# Day 1: Introduction to Deep Learning\n",
    "https://docs.google.com/presentation/d/1vxQ_R8-gEtR896EoC1_ObSKoWJZje40H8wcXzdltjpo/edit#slide=id.g9e8c78a5a9_0_18\n",
    "\n",
    "### What We’re Going to Learn\n",
    "- High Level Overview of Deep Learning\n",
    "- Applications of Deep Learning \n",
    "- What is a Neural Network\n",
    "\n",
    "\n",
    "## Deep Learning\n",
    "- Deep learning is a **subfield of machine learning**\n",
    "- Deep learning is **all about neural networks**\n",
    "- Deep learning **refers to large neural networks**\n",
    "- Neural networks have been around for a while but is only relatively recently that we have the computing power to train and run the these larger more complex neural networks\n",
    "- The deep part comes from **having many layers in the networks**\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-1.png?raw=true\" width=\"700\">\n",
    "\n",
    "### Why Deep Learning?\n",
    "*“The analogy to deep learning is that the rocket engine is the deep learning models and the fuel is the huge amounts of data we can feed to these algorithms.”* - Andrew Ng\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-2.png?raw=true\" width=\"500\">\n",
    "\n",
    "### Drawbacks of Deep Learning\n",
    "- Interpretability is harder, very black box \n",
    "- Doesn’t work well with small datasets\n",
    "    - the less data you have, the worse it's going to perform\n",
    "    - more instances/samples (~10,000) not how many features\n",
    "- Needs lots of computing power\n",
    "\n",
    "## Neural Networks\n",
    "- Modeled loosely on the human brain\n",
    "- Composed of many simple processing nodes or “neurons”\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-3.png?raw=true\" width=\"700\">\n",
    "\n",
    "## A Neuron\n",
    "- For most modern deep learning algorithms we use what is called a “sigmoid neuron”\n",
    "- In order to understand these better let’s take a look at the idea they were based on, a perceptron, one of the first types of artificial neurons\n",
    "\n",
    "## Perceptrons\n",
    "- A perceptron takes in multiple inputs and produces a single binary output\n",
    "- Each of these inputs x have a weight w, what would weights do?\n",
    "\t◦ Weights are used to add importance of a feature, giving it more influence on the output\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-4.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "## Deep Neural Networks\n",
    "We add all those x*w’s together to get a sum\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-5.png?raw=true\" width=\"500\">\n",
    "\n",
    "\n",
    "## Activation Function //58m\n",
    "How do we know whether to give a 0 or 1, that’s the activation functions job! One example is the Unit Step Activation Function\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-6.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "### What are some drawbacks of this decision making model? //1h1m\n",
    "- Setting an absolute threshold is very not a good decision￼\n",
    "\n",
    "\n",
    "## Sigmoid Neuron: Much Less Harsh //1h4m\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-7.png?raw=true\" width=\"700\">\n",
    "\n",
    "## Sigmoid Neuron Output //1h5\n",
    "\n",
    "- Our output is no longer 0 or 1\n",
    "- It is now a value from 0 - 1 and can be interpreted like a probability\n",
    "- We now have more control over how that output is interpreted\n",
    "\n",
    "## Putting it all together\n",
    "- A neural network is really just a bunch of interconnected neurons, the output from one layer or neurons can then be used as one the inputs for the next layer\n",
    "- Through a series of layers different patterns of neurons are “activated” producing different end results for different inputs\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day1-8.png?raw=true\" width=\"700\">\n",
    "\n",
    "## Next Steps\n",
    "- Math happening behind the scenes?\n",
    "- What about those algorithms?\n",
    "- How does it actually get the inputs (features)?\n",
    "- What sorts of parameters and choices can we mess with, how do we optimize?\n",
    "- How do we choose how many layers and neurons?\n",
    "- What tools are available to us?\n",
    "- How can we apply this to a real world problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day2\"></a> \n",
    "# Day 2 What is Neural Network?\n",
    "https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Lessons/WhatisNeuralNetwork.md\n",
    "\n",
    "### Learning Objectives\n",
    "\t•\tWhat is Neural network\n",
    "\t•\tWe learn Forward and backward propagation in NN\n",
    "\t•\tWe implement a NN entirely in numpy\n",
    "\n",
    "## What is Neural Network?\n",
    "It is a **computational system inspired by the Structure, Processing Method and Learning Ability similar to our biological brain**\n",
    "\n",
    "### Characteristics of Artificial Neural Networks\n",
    "\t•\tA large number of very simple processing neuron-like processing elements\n",
    "\t•\tA large number of weighted connections between the elements\n",
    "\t•\tDistributed representation of knowledge over the connections\n",
    "\t•\tKnowledge is acquired by network through a learning process\n",
    "\n",
    "## What is perceptron?\n",
    "\t•\tA perceptron can be understood as anything that **takes multiple inputs and produces one output**\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day2-1.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "## Multi-layer perceptron (MLP)\n",
    "\t•\tMLP is the stack of perceptrons\n",
    "    \n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day2-2.png?raw=true\" width=\"700\">\n",
    "\n",
    "\t•\tIn this image, the yellow nodes are inputs, the blue nodes (at each vertical) are hidden layers and the orange ones are output of the MLP\n",
    "\n",
    "## Forward and backward propagation\n",
    "NN **takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer.** This result estimation process is technically known as **“Forward Propagation“**\n",
    "\n",
    "Next, we **compare the result with actual output**. The task is to make the output to neural network as close to actual (desired) output. This defines our cost function.\n",
    "We try to **obtain the weight of neurons such that the NN total error (our cost function) being minimized**. This process is known as **“Backward Propagation“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#creating a vector\n",
    "vector = np.array([5,2,1])\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98231977]\n",
      " [0.97394954]\n",
      " [0.03823383]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/\n",
    "# Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "\n",
    "#Sigmoid Function //gives us a value between 0 and 1 or Probability\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "\n",
    "#Variable initialization\n",
    "epoch=5000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "    D = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = D * slope_output_layer\n",
    "    Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *lr\n",
    "    bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "    wh += X.T.dot(d_hiddenlayer) *lr\n",
    "    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we update the weights to minimize the error?\n",
    "1. First we should define the cost function. for our example here the MSE is our cost function:\n",
    "\n",
    "$E= \\frac{1}{2} ({\\bf y}_t - {\\bf y}_p)^T ({\\bf y}_t - {\\bf y}_p)$\n",
    "\n",
    "2. We update the weight (${\\bf W}_i$ and ${\\bf W}_h$) such that the error, $E$, being minimized. The most popular algorithm is Gradient Descent:\n",
    "\n",
    "${\\bf W}_h = {\\bf W}_h + \\eta {\\partial E}/{\\partial {\\bf W}_h} $\n",
    "\n",
    "For our above example we can show that:\n",
    "\n",
    "${\\partial E}/{\\partial {\\bf W}_h} = ({\\bf y}_t - {\\bf y}_p) {\\bf y}_p (1 - {\\bf y}_p)\\bf {h}$\n",
    "\n",
    "where ${\\bf h} = \\sigma({\\bf W}_i {\\bf x}_i + {\\bf b}_i)$\n",
    "\n",
    "In above code:\n",
    "$D = {\\bf y}_t - {\\bf y}_p$\n",
    "\n",
    "${\\bf y}_p (1 - {\\bf y}_p)$ = slope_hidden_layer\n",
    "\n",
    "$\\bf {h}$ = hiddenlayer_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Resources:\n",
    "- https://enlight.nyc/projects/neural-network/\n",
    "- WTF is Tensor: https://www.kdnuggets.com/2018/05/wtf-tensor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"day3\"></a> \n",
    "# Day 3: Neural Network from Scratch\n",
    "https://docs.google.com/presentation/d/1QstriVSuk8ZXWYjPrJ6SZUTjjYTC9Z6HHwJAun-hBEE/edit#slide=id.p\n",
    "\n",
    "### Interview Questions\n",
    "- What is the bias variance tradeoff in machine learning?\n",
    "    - Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance\n",
    "    - https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/#:~:text=Bias%20is%20the%20simplifying%20assumptions,the%20bias%20and%20the%20variance.\n",
    "\n",
    "- What is an example of a low bias ML algorithm?\n",
    "    - Low Variance: Suggests small changes to the estimate of the target function with changes to the training dataset.\n",
    "        - Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "    - High Variance: Suggests large changes to the estimate of the target function with changes to the training dataset.\n",
    "        - Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines.\n",
    "        - Can potentially overfit\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-1.png?raw=true\" width=\"750\">\n",
    "\n",
    "- Bias\n",
    "    - Shifts curve of our activation function\n",
    "    - Weights can help give a guidance towards your model\n",
    "    - Help optimize parameters to help\n",
    "    - Allows us to place significance over selected parameters\n",
    "    - Traditionally set to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: Machine Learning for Beginners: An Introduction to Neural Networks\n",
    "https://victorzhou.com/blog/intro-to-neural-networks/\n",
    "\n",
    "## Neuron\n",
    "- A neuron takes inputs, does some math with them, and produces one output\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-2.png?raw=true\" width=\"500\">\n",
    "\n",
    "1. First, each input is multiplied by a weight:\n",
    "\n",
    "    `x1 -> x1 * w1`\n",
    "    `x2 -> x2 * w2`\n",
    "    \n",
    "2. All the weighted inputs are added together with a bias *b*:\n",
    "    \n",
    "    `(x1 * w1) + (x2 * w2) + b`\n",
    "    \n",
    "3. The sum is passed through an activation function:\n",
    "\n",
    "    `y = f(x1*w1 + x2*w2 + b)`\n",
    "\n",
    "### Activation Function\n",
    "- used to turn an unbounded input into an output that has a nice, predictable form. \n",
    "- A commonly used activation function is the sigmoid function\n",
    "\n",
    "## Sigmoid\n",
    "- The sigmoid function only outputs numbers in the range (0,1). You can think of it as compressing (−∞,+∞) to (0,1) - big negative numbers become ~0, and big positive numbers become ~1.\n",
    "\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-3.png?raw=true\" width=\"500\">\n",
    "\n",
    "### Neuron Code in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"This class represents an articfical sigmoid neuron\"\"\"\n",
    "\n",
    "    def __init__(self, weights, bias, activation_function):\n",
    "        \"\"\" initializes this neuron with weights, inputs, and bias\"\"\"\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def feedForward(self, inputs):\n",
    "        \"\"\"steps to return the y output using dot function\"\"\"\n",
    "        result = np.dot(self.weights, inputs) + self.bias\n",
    "        return self.activation_function(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9933071490757153\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0,1])\n",
    "bias = 1\n",
    "inputs = np.array([2,4])\n",
    "#instantiate a neuron\n",
    "neuron = Neuron(weights, bias, sigmoid)\n",
    "print(neuron.feedForward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "### The Neural Network's FeedForward\n",
    "<img src=\"https://github.com/SamuelFolledo/DS2.2-Deep-Learning/blob/master/static/screenshots/day3-4.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "        #inputs or X's\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #hidden layer\n",
    "        self.h1 = Neuron(weights, bias, sigmoid)\n",
    "        self.h2 = Neuron(weights, bias, sigmoid)\n",
    "        \n",
    "        #output layer\n",
    "        self.o1 = Neuron(weights, bias, sigmoid)\n",
    "        \n",
    "    def feedForward(self, inputs):\n",
    "        \"\"\"get hidden layer outputs\"\"\"\n",
    "        out_h1 = self.h1.feedForward(inputs)\n",
    "        out_h2 = self.h2.feedForward(inputs)\n",
    "        \n",
    "        out_o1 = self.o1.feedForward(np.array([out_h1, out_h2]))\n",
    "        return out_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8800925786929503\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0, 1])\n",
    "bias = 1\n",
    "\n",
    "simpleNN = SimpleNeuralNetwork(weights, bias)\n",
    "inputs = np.array([2, 4])\n",
    "\n",
    "print(simpleNN.feedForward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW: \n",
    "Check out this [tutorial](https://victorzhou.com/blog/intro-to-neural-networks/) to prep for the backpropagation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

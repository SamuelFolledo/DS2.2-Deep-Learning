{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 What is Neural Network?\n",
    "https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/blob/master/Lessons/WhatisNeuralNetwork.md\n",
    "\n",
    "### Learning Objectives\n",
    "\t•\tWhat is Neural network\n",
    "\t•\tWe learn Forward and backward propagation in NN\n",
    "\t•\tWe implement a NN entirely in numpy\n",
    "\n",
    "## What is Neural Network?\n",
    "It is a computational system inspired by the Structure, Processing Method and Learning Ability similar to our biological brain\n",
    "\n",
    "### Characteristics of Artificial Neural Networks\n",
    "\t•\tA large number of very simple processing neuron-like processing elements\n",
    "\t•\tA large number of weighted connections between the elements\n",
    "\t•\tDistributed representation of knowledge over the connections\n",
    "\t•\tKnowledge is acquired by network through a learning process ",
    "\n",
    "\n",
    "## What is perceptron?\n",
    "\t•\tA perceptron can be understood as anything that takes multiple inputs and produces one output\n",
    "\n",
    "<img src=\"static/screenshots/day2-1.png\" width=\"700\">\n",
    "\n",
    "\n",
    "## Multi-layer perceptron (MLP)\n",
    "\t•\tMLP is the stack of perceptrons\n",
    "    \n",
    "<img src=\"static/screenshots/day2-2.png\" width=\"700\">\n",
    "\n",
    "\t•\tIn this image, the yellow nodes are inputs, the blue nodes (at each vertical) are hidden layers and the orange ones are output of the MLP\n",
    "\n",
    "## Forward and backward propagation\n",
    "NN takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer. This result estimation process is technically known as **“Forward Propagation“**\n",
    "\n",
    "Next, we compare the result with actual output. The task is to make the output to neural network as close to actual (desired) output. This defines our cost function.\n",
    "We try to obtain the weight of neurons such that the NN total error (our cost function) being minimized. This process is known as **“Backward Propagation“**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#creating a vector\n",
    "vector = np.array([5,2,1])\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

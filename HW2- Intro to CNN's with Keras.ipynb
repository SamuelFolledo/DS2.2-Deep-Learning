{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Intro to CNN's with Keras\n",
    "\n",
    "### Introduction\n",
    "In this notebook we'll be working with Convolutional Neural Networks (CNNs), a class of neural network that works particularly well with image data by filtering* (convolving). CNNs are a way to focus training on a subset of information, nullifying unimportant features. In theory, you can use a CNN wherever you use a vanilla neural net (with some restrictions). However, depending on your data, your mileage on speed and accuracy may vary. For images, we know CNNs tend to perform better and even for some speech problems a 1D CNN can be useful.\n",
    "\n",
    "We'll mainly be looking at image data, all similar to data we used in the *Introduction to Keras* notebook. That way, we may concentrate on learning CNNs rather than learn about new data.\n",
    "\n",
    "*note: By filtering, we mean, for example, adding a blurring effect (or a blur filter) onto an image. This might make understanding an image harder to us humans, but it makes it easier for a computer to understand and learn. Different filters work for di\n",
    "\n",
    "### A note about training:\n",
    "Training locally might take a long while. CNNs are more computationally expensive and we are dealing with deeper networks, meaning training is no longer fast. As an example, running locally the first CNN model in this notebook took a couple of minutes per epoch. Colab's GPU, under 15 seconds per epoch. I highly recommend you do this on a GPU to speed up your results rather than waste potentially hours training.\n",
    "\n",
    "If on Colab, make your runtimes with a GPU by clicking on Runtime, Change runtime, and click on the dropdown menu to GPU.\n",
    "\n",
    "## Revenge of Fashion MNIST\n",
    "In the previous notebook, we asked to train a simple neural network on a dataset known as Fashion MNIST. If you remember, getting results near or above the high 80%s was pretty difficult. Here we will show how CNNs fair when attacking this problem.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow import keras\n",
    "# I am assuming tensorflow is installed in your machine since the last \n",
    "# notebook. However, if you need it installed again, unmute and run below\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducible (and grading purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "#If the two lines above give you error, mute them and run the following:\n",
    "# import tensorflow\n",
    "# tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28)\n",
      "y_train: (60000,)\n",
      "X_test: (10000, 28, 28)\n",
      "y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading datasets\n",
    "fashion = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion.load_data()\n",
    "\n",
    "# Setting labels\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \n",
    "         \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# Seeing shapes of data\n",
    "print(\"X_train:\", X_train_full.shape)\n",
    "print(\"y_train:\", y_train_full.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you would like to see how this data looks like again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected value is: Trouser\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rerun to get different images\n",
    "def plotImage(image):\n",
    "    \"\"\"A 28x28 array that represents an image.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "sample_image = np.random.randint(0, len(X_train_full))\n",
    "print(\"The expected value is:\", labels[y_train_full[sample_image]])\n",
    "plotImage(X_train_full[sample_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting our data into Train / Validate / Test and normalizing/scale our data (preprocessing in order to obtain better results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255, X_train_full[5000:] / 255\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255\n",
    "#No need to do y_test since it's not being fed into the data.\n",
    "#Also, data is already shuffled, so no need to do so here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, CNNs input a 3D tensor (for a 2D convolution). The current shape of our input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is 2D, 28x28 (the 55000 is the number of images saved to the variable). So we reshape our input data using this trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_2D_to_3D(X):\n",
    "    \"\"\"Reshape variable from 2D to 3D of an X of shape (length, x, y) by adding \n",
    "    a 1 as a dimension in numpy's reshape function to out (length, x, y, color \n",
    "    channel).\n",
    "    \"\"\"\n",
    "    return np.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "X_train = reshape_2D_to_3D(X_train)\n",
    "X_valid = reshape_2D_to_3D(X_valid)\n",
    "X_test = reshape_2D_to_3D(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create a CNN\n",
    "\n",
    "## Sign Language MNIST\n",
    "\n",
    "### Loading and Some Preprocessing\n",
    "\n",
    "You're going to be using a sign language dataset to predict letters in the American sign language alphabet. The dataset can be downloaded through [kaggle](https://www.kaggle.com/datamunge/sign-language-mnist) but if you don't have a Kaggle account, you may download it through [here](https://drive.google.com/drive/folders/1miNKEZ4hbimO9dW87eW4j0vfv_RHQld1?usp=sharing).\n",
    "\n",
    "You're going to have to find a way to read in your data. Locally isn't much of an issue, however reading your data through Colab or Kaggle could be problematic. I've followed [this tutorial](https://towardsdatascience.com/google-colab-import-and-export-datasets-eccf801e2971) on getting data onto Colab and have muted the cells below, but please ignore them and look elsewhere to upload your data if you're not using Colab.\n",
    "\n",
    "### Problem 0: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     13     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "            'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "X_train_full = pd.read_csv('sign_mnist_train.csv') \n",
    "X_test_full = pd.read_csv('sign_mnist_train.csv') \n",
    "X_train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset containts 785 columns, 784 for pixels and one column for labels (alphabet). Let's extract our labels and drop the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_full = X_train_full['label']\n",
    "y_test_full = X_test_full['label']\n",
    "X_train_full.drop(labels=['label'], axis=1, inplace=True)\n",
    "X_test_full.drop(labels=['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use sklearn to shuffle and break our data into train/val. We previously didn't need to do this since our data came shuffled but in this case, it isn't. We'll also convert our pandas DataFrame to our more familiary numpy array. While training DataFrames are possible in Tensorflow, for consistency and flexibility, we'll stick with numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "# Feel free to change test size\n",
    "\n",
    "# Converting to numpy arrays\n",
    "def pandas_to_array(X, shape):\n",
    "  \"\"\"Convert a pandas array into numpy. Shape is in the form of a tuple\"\"\"\n",
    "  return X.to_numpy().reshape(len(X), *shape)\n",
    "\n",
    "shape = (28, 28)\n",
    "X_train = pandas_to_array(X_train, (shape))\n",
    "X_valid = pandas_to_array(X_valid, (shape))\n",
    "y_train = y_train.to_numpy()\n",
    "y_valid = y_valid.to_numpy()\n",
    "\n",
    "# Also converting our test data\n",
    "X_test = pandas_to_array(X_test_full, shape)\n",
    "y_test = y_test_full.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this to see our data. For reference, [here's a link to a sign language alphabet](https://www.startasl.com/wp-content/uploads/asl-alphabet_wallpaper_1920x1200.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected letter is: x\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rerun to get different images\n",
    "def plotImage(image):\n",
    "    \"\"\"A 28x28 array that represents an image.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "sample_image = np.random.randint(0, len(X_train))\n",
    "print(\"The expected letter is:\", alphabet[y_train[sample_image]])\n",
    "plotImage(X_train[sample_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about ASL: two of it's letters, j and z, are movements. As such, these letters are not represented in our dataset. As proof, finding all labels in y_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values 9 and 25 are missing from this set, which are both j and z respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "print(alphabet[9])\n",
    "print(alphabet[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "### Problem 0: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7172, 785)\n",
      "(27455, 785)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D, Flatten, Input\n",
    "# import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "\n",
    "df_test = pd.read_csv(\"sign_mnist_test.csv\")\n",
    "df_train = pd.read_csv(\"sign_mnist_train.csv\")\n",
    "df_train.head()\n",
    "\n",
    "print(df_test.shape)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Finish preprocessing\n",
    "\n",
    "Finish scaling the data and reshaping it so that it is appropriate for a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale data\n",
    "y = df_train[\"label\"]\n",
    "X = df_train.drop(['label'], axis=1)\n",
    "\n",
    "X = np.array(X)/255\n",
    "y = np.array(y)\n",
    "\n",
    "Y = np.zeros((len(alphabet),df_train.shape[0]))\n",
    "for i in range(len(y)):\n",
    "  Y[y[i],i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reshape data\n",
    "X = X.reshape((-1, 28,28,1))\n",
    "Y = Y.reshape((26,-1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Create a CNN model\n",
    "Create, compile, and train a CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create CNN, several cells added below in case needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 556)               1744172   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               71296     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 26)                3354      \n",
      "=================================================================\n",
      "Total params: 1,883,814\n",
      "Trainable params: 1,883,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1),padding='same'))\n",
    "model.add(tf.keras.layers.Convolution2D(32, (3, 3), activation='relu',padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2,2), strides=None,padding='same'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu',padding='same'))\n",
    "model.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu',padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2,2), strides=None,padding='same'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(556, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(26, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "344/344 [==============================] - 44s 129ms/step - loss: 1.1248 - accuracy: 0.6530 - val_loss: 0.0659 - val_accuracy: 0.9801\n",
      "Epoch 2/3\n",
      "344/344 [==============================] - 45s 130ms/step - loss: 0.0566 - accuracy: 0.9829 - val_loss: 0.0062 - val_accuracy: 0.9984\n",
      "Epoch 3/3\n",
      "344/344 [==============================] - 44s 127ms/step - loss: 0.0168 - accuracy: 0.9946 - val_loss: 0.0015 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history = model.fit(X,y,batch_size=64,epochs=3, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Analyze Error\n",
    "Produce a confusion matrix and report the most misrepresented label that your model perdicted. Could you give a hypothesis for why this label was so badly predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_valid = pd.read_csv(\"sign_mnist_test.csv\")\n",
    "\n",
    "#preprocessing\n",
    "n = rd.randrange(df_valid.shape[0])\n",
    "y = df_valid[\"label\"]\n",
    "X = df_valid.drop(['label'], axis=1)\n",
    "\n",
    "ar = np.array(df_valid.loc[n][1:]).reshape((28,28))\n",
    "\n",
    "X = np.array(X)/255\n",
    "y = np.array(y)\n",
    "\n",
    "Y = np.zeros((26,df_valid.shape[0]))\n",
    "for i in range(len(y)):\n",
    "  Y[y[i],i] = 1\n",
    "\n",
    "# reshape\n",
    "X = X.reshape((-1, 28,28,1))\n",
    "Y = Y.reshape((26,-1))\n",
    "\n",
    "plt.imshow(ar, cmap='gray')\n",
    "plt.title(f\"Prediction :  {alphabet[ np.argmax(model.predict(X[n].reshape(1,28,28,1)))]} | had to predict {alphabet[df_valid.loc[n][0]]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [26, 7172]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-b93e65c7a30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [26, 7172]"
     ]
    }
   ],
   "source": [
    "# TODO: Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y, np.argmax(model.predict(X), axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: If you're planning on making the colorplot, make labels=alphabet in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.xticks and plt.yticks. You may need to take out j and z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(ar, cmap='gray')\n",
    "plt.title(f\"Prediction :  {alphabet[ np.argmax(model.predict(X[n].reshape(1,28,28,1)))]} | had to predict {alphabet[df_valid.loc[n][0]]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Score your data\n",
    "Score your data using X_test. Are there signs of overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Score X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Problem 5: Predict Your Own Letter\n",
    "Now that you have a model, let's see if it can predict a letter you give it. Take a picture of you doing any letter (besides j and z) on a semi-light background (look at examples through the sign language dataset to get close to the format of how their image is taken) and convert it into a png file. Find the path of the image and input it into the function below. This function will convert an image into the MNIST format.\n",
    "\n",
    "You may need to play around with image paths if on something like Colab. Follow the same steps as loading the sign language dataset to get a path that can be routed to your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2894\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-857b02fcdbeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0myour_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: file path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myour_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-857b02fcdbeb>\u001b[0m in \u001b[0;36mimageprepare\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mimput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpng\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "# Credits to: https://stackoverflow.com/questions/35842274/convert-own-image-to-mnists-image\n",
    "# Function to convert a png location to a pixel-MNIST style image.\n",
    "from PIL import Image, ImageFilter #pip install if needed\n",
    "\n",
    "def imageprepare(argv):\n",
    "    \"\"\"\n",
    "    This function returns the pixel values.\n",
    "    The imput is a png file location.\n",
    "    \"\"\"\n",
    "    im = Image.open(argv).convert('L')\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    newImage = Image.new('L', (28, 28), (255))  # creates white canvas of 28x28 pixels\n",
    "\n",
    "    if width > height:  # check which dimension is bigger\n",
    "        # Width is bigger. Width becomes 20 pixels.\n",
    "        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n",
    "        if (nheight == 0):  # rare case but minimum is 1 pixel\n",
    "            nheight = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n",
    "        newImage.paste(img, (4, wtop))  # paste resized image on white canvas\n",
    "    else:\n",
    "        # Height is bigger. Heigth becomes 20 pixels.\n",
    "        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n",
    "        if (nwidth == 0):  # rare case but minimum is 1 pixel\n",
    "            nwidth = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n",
    "        newImage.paste(img, (wleft, 4))  # paste resized image on white canvas\n",
    "\n",
    "    # newImage.save(\"sample.png\n",
    "\n",
    "    tv = list(newImage.getdata())  # get pixel values\n",
    "\n",
    "    # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    "    print(tva)\n",
    "    return np.array(tva).reshape(28, 28, 1)\n",
    "\n",
    "\n",
    "\n",
    "your_image = imageprepare('') # TODO: file path here\n",
    "print(len(your_image.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with your model your image\n",
    "# model.predict(your_image) # Change model to whatever you named your model to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your model predict correctly? If not, why do you think so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Once you're ready to submit, turn in a copy of this Jupyter notebook and a PDF copy of it. For the PDF, please label the pages of problems 1, 2, 3, 4, and (optionally) 5.\n",
    "\n",
    "Locally, there should a \"Download as\" under \"File\" and PDF should be an option. \n",
    "\n",
    "If you're doing this assignment using Colab, Colab does not have this PDF option. I recommend downloading and openning this notebook using IPython/Jupyter Notebook locally and doing the instruction above (with the automatic saved inputs). If you're resistant to this option, there are other options. If you're on a Mac, a hack you can do is to print this page and before you print, save the file as a PDF instead of printing. \n",
    "\n",
    "An alternative is to save your Colab notebook as an HTML file and convert that into a PDF (using a third party website like https://html2pdf.com/ or a browser extension like https://chrome.google.com/webstore/detail/web-page-to-pdf-converter/bbfoccanbdeldjaelafmbgonagegdndg). Converting HTMLs into PDFs tends to be easier/more common than ipynb to PDFs. If you need additional help, please feel free to Slack Kevin or Jess for aid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Intro to CNN's with Keras\n",
    "\n",
    "### Introduction\n",
    "In this notebook we'll be working with Convolutional Neural Networks (CNNs), a class of neural network that works particularly well with image data by filtering* (convolving). CNNs are a way to focus training on a subset of information, nullifying unimportant features. In theory, you can use a CNN wherever you use a vanilla neural net (with some restrictions). However, depending on your data, your mileage on speed and accuracy may vary. For images, we know CNNs tend to perform better and even for some speech problems a 1D CNN can be useful.\n",
    "\n",
    "We'll mainly be looking at image data, all similar to data we used in the *Introduction to Keras* notebook. That way, we may concentrate on learning CNNs rather than learn about new data.\n",
    "\n",
    "*note: By filtering, we mean, for example, adding a blurring effect (or a blur filter) onto an image. This might make understanding an image harder to us humans, but it makes it easier for a computer to understand and learn. Different filters work for di\n",
    "\n",
    "### A note about training:\n",
    "Training locally might take a long while. CNNs are more computationally expensive and we are dealing with deeper networks, meaning training is no longer fast. As an example, running locally the first CNN model in this notebook took a couple of minutes per epoch. Colab's GPU, under 15 seconds per epoch. I highly recommend you do this on a GPU to speed up your results rather than waste potentially hours training.\n",
    "\n",
    "If on Colab, make your runtimes with a GPU by clicking on Runtime, Change runtime, and click on the dropdown menu to GPU.\n",
    "\n",
    "## Revenge of Fashion MNIST\n",
    "In the previous notebook, we asked to train a simple neural network on a dataset known as Fashion MNIST. If you remember, getting results near or above the high 80%s was pretty difficult. Here we will show how CNNs fair when attacking this problem.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow import keras\n",
    "# I am assuming tensorflow is installed in your machine since the last \n",
    "# notebook. However, if you need it installed again, unmute and run below\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducible (and grading purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)\n",
    "#If the two lines above give you error, mute them and run the following:\n",
    "# import tensorflow\n",
    "# tensorflow.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28)\n",
      "y_train: (60000,)\n",
      "X_test: (10000, 28, 28)\n",
      "y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading datasets\n",
    "fashion = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion.load_data()\n",
    "\n",
    "# Setting labels\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \n",
    "         \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# Seeing shapes of data\n",
    "print(\"X_train:\", X_train_full.shape)\n",
    "print(\"y_train:\", y_train_full.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you would like to see how this data looks like again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected value is: Trouser\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rerun to get different images\n",
    "def plotImage(image):\n",
    "    \"\"\"A 28x28 array that represents an image.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "sample_image = np.random.randint(0, len(X_train_full))\n",
    "print(\"The expected value is:\", labels[y_train_full[sample_image]])\n",
    "plotImage(X_train_full[sample_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting our data into Train / Validate / Test and normalizing/scale our data (preprocessing in order to obtain better results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255, X_train_full[5000:] / 255\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255\n",
    "#No need to do y_test since it's not being fed into the data.\n",
    "#Also, data is already shuffled, so no need to do so here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, CNNs input a 3D tensor (for a 2D convolution). The current shape of our input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is 2D, 28x28 (the 55000 is the number of images saved to the variable). So we reshape our input data using this trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_2D_to_3D(X):\n",
    "    \"\"\"Reshape variable from 2D to 3D of an X of shape (length, x, y) by adding \n",
    "    a 1 as a dimension in numpy's reshape function to out (length, x, y, color \n",
    "    channel).\n",
    "    \"\"\"\n",
    "    return np.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1))\n",
    "X_train = reshape_2D_to_3D(X_train)\n",
    "X_valid = reshape_2D_to_3D(X_valid)\n",
    "X_test = reshape_2D_to_3D(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create a CNN\n",
    "\n",
    "## Sign Language MNIST\n",
    "\n",
    "### Loading and Some Preprocessing\n",
    "\n",
    "You're going to be using a sign language dataset to predict letters in the American sign language alphabet. The dataset can be downloaded through [kaggle](https://www.kaggle.com/datamunge/sign-language-mnist) but if you don't have a Kaggle account, you may download it through [here](https://drive.google.com/drive/folders/1miNKEZ4hbimO9dW87eW4j0vfv_RHQld1?usp=sharing).\n",
    "\n",
    "You're going to have to find a way to read in your data. Locally isn't much of an issue, however reading your data through Colab or Kaggle could be problematic. I've followed [this tutorial](https://towardsdatascience.com/google-colab-import-and-export-datasets-eccf801e2971) on getting data onto Colab and have muted the cells below, but please ignore them and look elsewhere to upload your data if you're not using Colab.\n",
    "\n",
    "### Problem 0: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b79d998bf33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/gdrive/My Drive/archive/sign_mnist_train/sign_mnist_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-46f7fb3884e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n\u001b[1;32m      3\u001b[0m             'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/archive/sign_mnist_train/sign_mnist_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO: Change your path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/archive/sign_mnist_train/sign_mnist_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO: Chainge your path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/archive/sign_mnist_train/sign_mnist_train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "            'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "X_train_full = pd.read_csv('/content/gdrive/My Drive/archive/sign_mnist_train/sign_mnist_train.csv') #TODO: Change your path here\n",
    "X_test_full = pd.read_csv('/content/gdrive/My Drive/archive/sign_mnist_train/sign_mnist_train.csv') #TODO: Chainge your path here\n",
    "X_train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset containts 785 columns, 784 for pixels and one column for labels (alphabet). Let's extract our labels and drop the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e4352bde7083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_test_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "y_train_full = X_train_full['label']\n",
    "y_test_full = X_test_full['label']\n",
    "X_train_full.drop(labels=['label'], axis=1, inplace=True)\n",
    "X_test_full.drop(labels=['label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use sklearn to shuffle and break our data into train/val. We previously didn't need to do this since our data came shuffled but in this case, it isn't. We'll also convert our pandas DataFrame to our more familiary numpy array. While training DataFrames are possible in Tensorflow, for consistency and flexibility, we'll stick with numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ce07335047a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-ce07335047a4>\u001b[0m in \u001b[0;36mpandas_to_array\u001b[0;34m(X, shape)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpandas_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;34m\"\"\"Convert a pandas array into numpy. Shape is in the form of a tuple\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "# Feel free to change test size\n",
    "\n",
    "\n",
    "# Converting to numpy arrays\n",
    "def pandas_to_array(X, shape):\n",
    "  \"\"\"Convert a pandas array into numpy. Shape is in the form of a tuple\"\"\"\n",
    "  return X.to_numpy().reshape(len(X), *shape)\n",
    "\n",
    "shape = (28, 28)\n",
    "X_train = pandas_to_array(X_train, (shape))\n",
    "X_valid = pandas_to_array(X_valid, (shape))\n",
    "y_train = y_train.to_numpy()\n",
    "y_valid = y_valid.to_numpy()\n",
    "\n",
    "# Also converting our test data\n",
    "X_test = pandas_to_array(X_test_full, shape)\n",
    "y_test = y_test_full.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this to see our data. For reference, [here's a link to a sign language alphabet](https://www.startasl.com/wp-content/uploads/asl-alphabet_wallpaper_1920x1200.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected letter is: f\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rerun to get different images\n",
    "def plotImage(image):\n",
    "    \"\"\"A 28x28 array that represents an image.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "sample_image = np.random.randint(0, len(X_train))\n",
    "print(\"The expected letter is:\", alphabet[y_train[sample_image]])\n",
    "plotImage(X_train[sample_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about ASL: two of it's letters, j and z, are movements. As such, these letters are not represented in our dataset. As proof, finding all labels in y_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values 9 and 25 are missing from this set, which are both j and z respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "print(alphabet[9])\n",
    "print(alphabet[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Finish preprocessing\n",
    "\n",
    "Finish scaling the data and reshaping it so that it is appropriate for a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Create a CNN model\n",
    "Create, compile, and train a CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create CNN, several cells added below in case needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Analyze Error\n",
    "Produce a confusion matrix and report the most misrepresented label that your model perdicted. Could you give a hypothesis for why this label was so badly predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: If you're planning on making the colorplot, make labels=alphabet in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.xticks and plt.yticks. You may need to take out j and z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Score your data\n",
    "Score your data using X_test. Are there signs of overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Score X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Problem 5: Predict Your Own Letter\n",
    "Now that you have a model, let's see if it can predict a letter you give it. Take a picture of you doing any letter (besides j and z) on a semi-light background (look at examples through the sign language dataset to get close to the format of how their image is taken) and convert it into a png file. Find the path of the image and input it into the function below. This function will convert an image into the MNIST format.\n",
    "\n",
    "You may need to play around with image paths if on something like Colab. Follow the same steps as loading the sign language dataset to get a path that can be routed to your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2894\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-857b02fcdbeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0myour_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: file path here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myour_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-857b02fcdbeb>\u001b[0m in \u001b[0;36mimageprepare\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mimput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpng\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2895\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "# Credits to: https://stackoverflow.com/questions/35842274/convert-own-image-to-mnists-image\n",
    "# Function to convert a png location to a pixel-MNIST style image.\n",
    "from PIL import Image, ImageFilter #pip install if needed\n",
    "\n",
    "def imageprepare(argv):\n",
    "    \"\"\"\n",
    "    This function returns the pixel values.\n",
    "    The imput is a png file location.\n",
    "    \"\"\"\n",
    "    im = Image.open(argv).convert('L')\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    newImage = Image.new('L', (28, 28), (255))  # creates white canvas of 28x28 pixels\n",
    "\n",
    "    if width > height:  # check which dimension is bigger\n",
    "        # Width is bigger. Width becomes 20 pixels.\n",
    "        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n",
    "        if (nheight == 0):  # rare case but minimum is 1 pixel\n",
    "            nheight = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n",
    "        newImage.paste(img, (4, wtop))  # paste resized image on white canvas\n",
    "    else:\n",
    "        # Height is bigger. Heigth becomes 20 pixels.\n",
    "        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n",
    "        if (nwidth == 0):  # rare case but minimum is 1 pixel\n",
    "            nwidth = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n",
    "        newImage.paste(img, (wleft, 4))  # paste resized image on white canvas\n",
    "\n",
    "    # newImage.save(\"sample.png\n",
    "\n",
    "    tv = list(newImage.getdata())  # get pixel values\n",
    "\n",
    "    # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    "    print(tva)\n",
    "    return np.array(tva).reshape(28, 28, 1)\n",
    "\n",
    "\n",
    "\n",
    "your_image = imageprepare('') # TODO: file path here\n",
    "print(len(your_image.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with your model your image\n",
    "# model.predict(your_image) # Change model to whatever you named your model to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your model predict correctly? If not, why do you think so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Once you're ready to submit, turn in a copy of this Jupyter notebook and a PDF copy of it. For the PDF, please label the pages of problems 1, 2, 3, 4, and (optionally) 5.\n",
    "\n",
    "Locally, there should a \"Download as\" under \"File\" and PDF should be an option. \n",
    "\n",
    "If you're doing this assignment using Colab, Colab does not have this PDF option. I recommend downloading and openning this notebook using IPython/Jupyter Notebook locally and doing the instruction above (with the automatic saved inputs). If you're resistant to this option, there are other options. If you're on a Mac, a hack you can do is to print this page and before you print, save the file as a PDF instead of printing. \n",
    "\n",
    "An alternative is to save your Colab notebook as an HTML file and convert that into a PDF (using a third party website like https://html2pdf.com/ or a browser extension like https://chrome.google.com/webstore/detail/web-page-to-pdf-converter/bbfoccanbdeldjaelafmbgonagegdndg). Converting HTMLs into PDFs tends to be easier/more common than ipynb to PDFs. If you need additional help, please feel free to Slack Kevin or Jess for aid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
